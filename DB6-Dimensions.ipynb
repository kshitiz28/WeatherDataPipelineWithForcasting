{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f465abef-630b-478e-ae3b-c533486bed82","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_dim_date(start_date, end_date):\n    date_list = spark.sql(f\"select sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day ) as dates \").collect()[0].dates\n    \n    date_df = spark.createDataFrame([(d,) for d in date_list], [\"date_alt_key\"])\n    \n    date_df = date_df.withColumn('date_id', date_format(col(\"date_alt_key\"), \"yyyyMMdd\"))\\\n                     .withColumn('year', year(\"date_alt_key\")) \\\n                     .withColumn('quarter', quarter(\"date_alt_key\")) \\\n                     .withColumn('month', month(\"date_alt_key\")) \\\n                     .withColumn('day', dayofmonth(\"date_alt_key\")) \\\n                     .withColumn('day_of_week', date_format(col(\"date_alt_key\"), 'EEEE'))\n    \n    date_df = date_df.select('date_id', 'date_alt_key', 'year', 'quarter', 'month', 'day', 'day_of_week')\n    \n    date_df.write.format('delta').mode('overwrite').saveAsTable('dim_date')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"76d8636c-e16c-4c73-9ef5-d96caa403a21","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_dim_time(start_time, end_time, interval):\n    time_list = spark.sql(f\"select sequence(to_timestamp('{start_time}', 'HH:mm:ss'), to_timestamp('{end_time}','HH:mm:ss'), interval '{interval}')\\\n                    AS time_alt\").collect()[0].time_alt\n    time_df = spark.createDataFrame([(t,) for t in time_list], ['time_alt'])\n    time_df = time_df.withColumn('time_alt_key', date_format(col('time_alt'), 'HH:mm:ss'))\n    \n    windowSpec = Window.orderBy('time_alt_key')\n    time_df = time_df.withColumn('time_id', row_number().over(windowSpec).cast(IntegerType()))\n    time_df = time_df.select('time_id', 'time_alt_key')\n    \n    time_df.write.format('delta').mode('overwrite').saveAsTable('dim_time')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"68608d1b-1129-48dd-bfd7-867a32249bbd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_dim_city():\n    city_df = spark.read.format('delta').load('dbfs:/FileStore/shared_uploads/DeltaTablecities')\n    city_df = city_df.select('id','name','lat','lon').limit(5)\n    \n    city_df.write.format('delta').mode('overwrite').saveAsTable('dim_city')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"176a3078-d804-4093-8148-dc582c36eee0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DB6-Dimensions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
